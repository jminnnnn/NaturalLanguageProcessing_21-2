{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss Calculation for each n-gram\n",
    "\n",
    "## To-Do\n",
    "- Language Modeling에 따른 Cross Entropy 구현\n",
    "- N-gram Generation, Word Counting, Cleaning Revisit\n",
    "- Smoothing (Add-1) implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "In Information Theory, entropy (denoted $H(X)$) of a random variable X is the expected log probabiltiy:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(X) = - \\sum P(x)log_2 P(x)\n",
    "\\end{equation}\n",
    "\n",
    "and is a measure of uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defn: Cross Entropy\n",
    "\n",
    "The cross entropy, H(p,m), of a true distribution **p** and a model distribution **m** is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(p,m) = - \\sum_{x} p(x) log_2 m(x)\n",
    "\\end{equation}\n",
    "\n",
    "The lower the cross entropy is the closer it is to the true distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- NIRW1900000011.json 은 전자신문 뉴스기사이다. (training data로 사용)\n",
    "- NWRW1800000045.json 은 동아일보 뉴스 기사이다. (test data로 사용)\n",
    "- 국립국어원의 웹 코퍼스 (WEB) 중의 하나인 EBRW1908000138.json (첨부)은 블로그 자료이다. (test data로 사용)\n",
    "- training data에서 학습한 한글 자소/글자(음절)/어절 별 unigram, Bigram, trigram 모델이 같은 신문기사와 웹자료에 얼마나 잘 부합하는지를 교차 엔트로피로 살펴봄\n",
    "- 세 데이터에서 \"form\"에 해당하는 부분만을 각각 추출하여, 한글 글자들만 남긴 후 (스페이스도 고려) unigram, bigram, trigram 구성을 만들고 빈도를 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to consider\n",
    "- training 코퍼스에서는 entropy와 cross entropy는 같고 따라서 그 차이는 0이다\n",
    "- 코퍼스에서 테스트 하기 위한 테스트 코퍼스의 교차엔트로피는 각 모델의 확률을 구하고 이를 교차 엔트로피 공식에 따라 구하면 되는데, **이 경우 P(x)는 이 test 코퍼스의 자소별/글자별/어절별 unigram/bigram/trigram의 확률이고 모델의 확률인 logp(m)은 training 코퍼스인 코퍼스에서 구해진 각 모델의 확률이다.** 각 글자별로 이를 다 곱해서 더 하면 교차엔트로피가 구해진다. 즉 training테스트에서 설정한 언어모델이 test 코퍼스에 더 부합할수록 test에서의 각 구성의 확률이 training의 해당 확률에 근접하게 될 것임. 완벽한 경우 두 모델이 일치한다면, 즉 교차엔트로피가 실제 엔트로피와 동일하게 되면 그 차이는 0이 된다. 따라서 H(P,m) - H(p)의 차이가 작을수록 더 좋은 모델이 된다. \n",
    "- 이 경우 training 코퍼스에 없는 n-gram 구성이  test 코퍼스에 있을 경우 문제가 되니 이 구성의 확률을 얻기 위해 ADD-1을 사용해서 smoothing하라.\n",
    "(힌트: training 데이터의 각 n-gram모델의 구성과 test-data의 n-gram모델의 구성을 비교하여 빠져 있는 구성을 보충하고 add-1을 사용해서 확률을 구함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반복 사용 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from jamo import h2j, j2hcj\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Preprocessing 함수\n",
    "\"\"\"\n",
    ":param file: 전처리하고자 하는 json file\n",
    ":param n: n-gram method를 사용 (default = 1)\n",
    ":reutrn: word_dict, letter_dict, jamo_dict; 단어, 글자, 자모의 dcitionary\n",
    "\"\"\"\n",
    "def preprocessing(file, n=1):\n",
    "    with open(file, 'rt', encoding='UTF8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    word_dict = {}\n",
    "    letter_dict = {}\n",
    "    jamo_dict = {}\n",
    "    for i in range(len(data['document'])):\n",
    "        each_document = data['document'][i]\n",
    "        for j in range(len(each_document['paragraph'])):\n",
    "            #유니코드 상 예외 사항을 공백(space)로 대체\n",
    "            each_pragraph = each_document['paragraph'][j]\n",
    "            #form 안에 존재하는 유니코드 상 예외 사항 및 기타 오류를 공백(space)으로 대체\n",
    "            each_pragraph_preprocessed = re.sub('[^ ㄱ-ㅣ가-힣]', ''\n",
    "                                                , re.sub(\"[\\u3000\\t\\n\\ax03]\", \"\", each_pragraph['form']).replace('”',' ').replace('=', ' '))\n",
    "            temp_word_list = re.findall('\\w+', each_pragraph_preprocessed)\n",
    "            temp_word_dict = dict(Counter(ngrams(temp_word_list, n)))\n",
    "            word_dict = update_with_increment(word_dict, temp_word_dict)\n",
    "            temp_letter_list = []\n",
    "            words_preprocessed = each_pragraph_preprocessed.split(' ')\n",
    "            for k in range(len(words_preprocessed)):\n",
    "                each_word_preprocessed = words_preprocessed[k]\n",
    "                #띄어쓰기 단위로 글자와 자모의 ngram을 수행함\n",
    "                temp_letter_list_by_words = []\n",
    "                temp_jamo_list_by_words = []                \n",
    "                for l in range(len(list(each_word_preprocessed))):\n",
    "                    each_letter_preprocessed = list(each_word_preprocessed)[l]\n",
    "                    temp_letter_list_by_words.append(each_letter_preprocessed)\n",
    "                    temp_jamo_list = []\n",
    "                    each_jamo_preprocessed = list(j2hcj(h2j(each_letter_preprocessed)))\n",
    "                    if len(each_jamo_preprocessed) == 2:\n",
    "                        each_jamo_preprocessed.append('X')\n",
    "                    temp_jamo_list_by_words.append(each_jamo_preprocessed)\n",
    "                temp_letter_dict = dict(Counter(ngrams(temp_letter_list_by_words, n)))\n",
    "                letter_dict = update_with_increment(letter_dict, temp_letter_dict)\n",
    "                flatten_temp_jamo_list_by_words = flatten(temp_jamo_list_by_words)\n",
    "                temp_jamo_dict = dict(Counter(ngrams(flatten_temp_jamo_list_by_words, n)))\n",
    "                jamo_dict = update_with_increment(jamo_dict, temp_jamo_dict)    \n",
    "                        \n",
    "    return word_dict, letter_dict, jamo_dict\n",
    "\n",
    "\n",
    "#Ngram 함수\n",
    "\"\"\"\n",
    ":param lst: n-gram을 하고자하는 corpus의 list ex: ['ㄱ', 'ㅏ', 'ㅇ']\n",
    ":param n: 'N' of n-gram method\n",
    ":return: tuple, n-gram으로 만들어진 튜플\n",
    "\"\"\"\n",
    "from itertools import tee, islice\n",
    "def ngrams(lst, n):\n",
    "    tlst = lst\n",
    "    while True:\n",
    "        a, b = tee(tlst)\n",
    "        l = tuple(islice(a, n))\n",
    "        if len(l) == n:\n",
    "            yield l\n",
    "            next(b)\n",
    "            tlst = b\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "            \n",
    "#List를 word 단위로 정리하기 위한 함수(flatten nested list)\n",
    "\"\"\"\n",
    ":param t:, a netsted list\n",
    ":return: list, a flattened list \n",
    "\"\"\"\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "\n",
    "#중복되는 key의 값은 늘려주고 새로 생긴 key는 업데이트 해주는 함수\n",
    "\"\"\"\n",
    ":param dict1: preivous dictionary\n",
    ":param dict2: current dictionary\n",
    ":return: dict, updated dictionary\n",
    "\"\"\"\n",
    "def update_with_increment(dict1, dict2):\n",
    "    for i in range(len(dict2)):\n",
    "        tkey = list(dict2.keys())[i]\n",
    "        tvalue = dict2[tkey]\n",
    "        if tkey in dict1.keys():\n",
    "            dict1[tkey] += tvalue\n",
    "        else:\n",
    "            dict1[tkey] = tvalue\n",
    "    return dict1\n",
    "\n",
    "\n",
    "#빈도 dictionary를 확률로 바꿔주는 함수\n",
    "\"\"\"\n",
    ":param dictionary: 확률로 value를 업데이트할 dictionary\n",
    ":return: dict, 확률값으로 value가 변경된 dictionary(add-1 smoothing applied)\n",
    "\"\"\"\n",
    "def dict_to_prob(dictionary):\n",
    "    cdict = dictionary.copy()\n",
    "    #v value of add-1 smoothing\n",
    "    v = len(cdict)\n",
    "    total = sum(cdict.values())\n",
    "    for key, value in cdict.items():\n",
    "        cdict[key] = (value+1) / (total+v)\n",
    "    return cdict\n",
    "\n",
    "\n",
    "#entropy를 구하는 함수\n",
    "\"\"\"\n",
    ":param : X, frequency dictionary\n",
    ":return: float, entropy 값\n",
    "\"\"\"\n",
    "def entropy_calc(X):\n",
    "    dist_X = dict_to_prob(X)\n",
    "    entropy = 0\n",
    "    for value in dist_X.values():\n",
    "        entropy += -(value*math.log(value, 2))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "#model distribution과 true distribution의 cross entropy를 구하는 함수\n",
    "\"\"\"\n",
    ":param X: n-gram actual frequency dictionary\n",
    ":param M: n-gram model frequency dictionary\n",
    ":return: cross_entropy, cross entropy between X and M\n",
    "\"\"\"\n",
    "def cross_entropy_calc(X, M):\n",
    "    dist_X = dict_to_prob(X)\n",
    "    dist_M = dict_to_prob(M)\n",
    "    total_X = len(X)\n",
    "    V_X = sum(X.values())\n",
    "    total_M = len(M)\n",
    "    V_M = sum(M.values())\n",
    "    cross_entropy = 0\n",
    "    allkey = set().union(*[X, M])\n",
    "    temp_dict_X = dict.fromkeys(list(allkey), 0)\n",
    "    temp_dict_M = dict.fromkeys(list(allkey), 0)\n",
    "    for key in temp_dict_X.keys():\n",
    "        if key in X.keys():\n",
    "            temp_dict_X[key] = dist_X[key]\n",
    "        else:\n",
    "            temp_dict_X[key] = (1)/(total_X + V_X)\n",
    "    for key in temp_dict_M.keys():\n",
    "        if key in M.keys():\n",
    "            temp_dict_M[key] = dist_M[key]\n",
    "        else:\n",
    "            temp_dict_M[key] = (1)/(total_M + V_M)\n",
    "    for key in temp_dict_X.keys():\n",
    "        cross_entropy += -(temp_dict_X[key]*math.log(temp_dict_M[key],2))\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, test1, test2의 어절, 글자, 자모음 별 빈도 계산(uni, bi, tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "word_dict_train_uni, letter_dict_train_uni, jamo_dict_train_uni = preprocessing('NIRW1900000011.json', 1)\n",
    "word_dict_test1_uni, letter_dict_test1_uni, jamo_dict_test1_uni = preprocessing('NWRW1800000045.json', 1)\n",
    "word_dict_test2_uni, letter_dict_test2_uni, jamo_dict_test2_uni = preprocessing('EBRW1908000138.json', 1)\n",
    "\n",
    "#bigram\n",
    "word_dict_train_bi, letter_dict_train_bi, jamo_dict_train_bi = preprocessing('NIRW1900000011.json', 2)\n",
    "word_dict_test1_bi, letter_dict_test1_bi, jamo_dict_test1_bi = preprocessing('NWRW1800000045.json', 2)\n",
    "word_dict_test2_bi, letter_dict_test2_bi, jamo_dict_test2_bi = preprocessing('EBRW1908000138.json', 2)\n",
    "\n",
    "#trigram\n",
    "word_dict_train_tri, letter_dict_train_tri, jamo_dict_train_tri = preprocessing('NIRW1900000011.json', 3)\n",
    "word_dict_test1_tri, letter_dict_test1_tri, jamo_dict_test1_tri = preprocessing('NWRW1800000045.json', 3)\n",
    "word_dict_test2_tri, letter_dict_test2_tri, jamo_dict_test2_tri = preprocessing('EBRW1908000138.json', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entropy, cross entropy, difference 계산(train, test1, test2) X (uni, bi, tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "entropy_word_train_uni = entropy_calc(word_dict_train_uni)\n",
    "entropy_letter_train_uni = entropy_calc(letter_dict_train_uni)\n",
    "entropy_jamo_train_uni = entropy_calc(jamo_dict_train_uni)\n",
    "\n",
    "cross_entropy_word_train_uni = cross_entropy_calc(word_dict_train_uni, word_dict_train_uni)\n",
    "cross_entropy_letter_train_uni = cross_entropy_calc(letter_dict_train_uni, letter_dict_train_uni)\n",
    "cross_entropy_jamo_train_uni = cross_entropy_calc(jamo_dict_train_uni, jamo_dict_train_uni)\n",
    "\n",
    "diff_word_uni_train = cross_entropy_word_train_uni - entropy_word_train_uni\n",
    "diff_letter_uni_train = cross_entropy_letter_train_uni - entropy_letter_train_uni\n",
    "diff_jamo_uni_train = cross_entropy_jamo_train_uni - entropy_jamo_train_uni\n",
    "\n",
    "#bigram\n",
    "entropy_word_train_bi = entropy_calc(word_dict_train_bi)\n",
    "entropy_letter_train_bi = entropy_calc(letter_dict_train_bi)\n",
    "entropy_jamo_train_bi = entropy_calc(jamo_dict_train_bi)\n",
    "\n",
    "cross_entropy_word_train_bi = cross_entropy_calc(word_dict_train_bi, word_dict_train_bi)\n",
    "cross_entropy_letter_train_bi = cross_entropy_calc(letter_dict_train_bi, letter_dict_train_bi)\n",
    "cross_entropy_jamo_train_bi = cross_entropy_calc(jamo_dict_train_bi, jamo_dict_train_bi)\n",
    "\n",
    "diff_word_bi_train = cross_entropy_word_train_bi - entropy_word_train_bi\n",
    "diff_letter_bi_train = cross_entropy_letter_train_bi - entropy_letter_train_bi\n",
    "diff_jamo_bi_train = cross_entropy_jamo_train_bi - entropy_jamo_train_bi\n",
    "\n",
    "#trigram\n",
    "entropy_word_train_tri = entropy_calc(word_dict_train_tri)\n",
    "entropy_letter_train_tri = entropy_calc(letter_dict_train_tri)\n",
    "entropy_jamo_train_tri = entropy_calc(jamo_dict_train_tri)\n",
    "\n",
    "cross_entropy_word_train_tri = cross_entropy_calc(word_dict_train_tri, word_dict_train_tri)\n",
    "cross_entropy_letter_train_tri = cross_entropy_calc(letter_dict_train_tri, letter_dict_train_tri)\n",
    "cross_entropy_jamo_train_tri = cross_entropy_calc(jamo_dict_train_tri, jamo_dict_train_tri)\n",
    "\n",
    "diff_word_tri_train = cross_entropy_word_train_tri - entropy_word_train_tri\n",
    "diff_letter_tri_train = cross_entropy_letter_train_tri - entropy_letter_train_tri\n",
    "diff_jamo_tri_train = cross_entropy_jamo_train_tri - entropy_jamo_train_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "entropy_word_test1_uni = entropy_calc(word_dict_test1_uni)\n",
    "entropy_letter_test1_uni = entropy_calc(letter_dict_test1_uni)\n",
    "entropy_jamo_test1_uni = entropy_calc(jamo_dict_test1_uni)\n",
    "\n",
    "cross_entropy_word_test1_uni = cross_entropy_calc(word_dict_test1_uni, word_dict_train_uni)\n",
    "cross_entropy_letter_test1_uni = cross_entropy_calc(letter_dict_test1_uni, letter_dict_train_uni)\n",
    "cross_entropy_jamo_test1_uni = cross_entropy_calc(jamo_dict_test1_uni, jamo_dict_train_uni)\n",
    "\n",
    "diff_word_uni_test1 = cross_entropy_word_test1_uni - entropy_word_test1_uni\n",
    "diff_letter_uni_test1 = cross_entropy_letter_test1_uni - entropy_letter_test1_uni\n",
    "diff_jamo_uni_test1 = cross_entropy_jamo_test1_uni - entropy_jamo_test1_uni\n",
    "\n",
    "#bigram\n",
    "entropy_word_test1_bi = entropy_calc(word_dict_test1_bi)\n",
    "entropy_letter_test1_bi = entropy_calc(letter_dict_test1_bi)\n",
    "entropy_jamo_test1_bi = entropy_calc(jamo_dict_test1_bi)\n",
    "\n",
    "cross_entropy_word_test1_bi = cross_entropy_calc(word_dict_test1_bi, word_dict_train_bi)\n",
    "cross_entropy_letter_test1_bi = cross_entropy_calc(letter_dict_test1_bi, letter_dict_train_bi)\n",
    "cross_entropy_jamo_test1_bi = cross_entropy_calc(jamo_dict_test1_bi, jamo_dict_train_bi)\n",
    "\n",
    "diff_word_bi_test1 = cross_entropy_word_test1_bi - entropy_word_test1_bi\n",
    "diff_letter_bi_test1 = cross_entropy_letter_test1_bi - entropy_letter_test1_bi\n",
    "diff_jamo_bi_test1 = cross_entropy_jamo_test1_bi - entropy_jamo_test1_bi\n",
    "\n",
    "#trigram\n",
    "entropy_word_test1_tri = entropy_calc(word_dict_test1_tri)\n",
    "entropy_letter_test1_tri = entropy_calc(letter_dict_test1_tri)\n",
    "entropy_jamo_test1_tri = entropy_calc(jamo_dict_test1_tri)\n",
    "\n",
    "cross_entropy_word_test1_tri = cross_entropy_calc(word_dict_test1_tri, word_dict_train_tri)\n",
    "cross_entropy_letter_test1_tri = cross_entropy_calc(letter_dict_test1_tri, letter_dict_train_tri)\n",
    "cross_entropy_jamo_test1_tri = cross_entropy_calc(jamo_dict_test1_tri, jamo_dict_train_tri)\n",
    "\n",
    "diff_word_tri_test1 = cross_entropy_word_test1_tri - entropy_word_test1_tri\n",
    "diff_letter_tri_test1 = cross_entropy_letter_test1_tri - entropy_letter_test1_tri\n",
    "diff_jamo_tri_test1 = cross_entropy_jamo_test1_tri - entropy_jamo_test1_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "entropy_word_test2_uni = entropy_calc(word_dict_test2_uni)\n",
    "entropy_letter_test2_uni = entropy_calc(letter_dict_test2_uni)\n",
    "entropy_jamo_test2_uni = entropy_calc(jamo_dict_test2_uni)\n",
    "\n",
    "cross_entropy_word_test2_uni = cross_entropy_calc(word_dict_test2_uni, word_dict_train_uni)\n",
    "cross_entropy_letter_test2_uni = cross_entropy_calc(letter_dict_test2_uni, letter_dict_train_uni)\n",
    "cross_entropy_jamo_test2_uni = cross_entropy_calc(jamo_dict_test2_uni, jamo_dict_train_uni)\n",
    "\n",
    "diff_word_uni_test2 = cross_entropy_word_test2_uni - entropy_word_test2_uni\n",
    "diff_letter_uni_test2 = cross_entropy_letter_test2_uni - entropy_letter_test2_uni\n",
    "diff_jamo_uni_test2 = cross_entropy_jamo_test2_uni - entropy_jamo_test2_uni\n",
    "\n",
    "#bigram\n",
    "entropy_word_test2_bi = entropy_calc(word_dict_test2_bi)\n",
    "entropy_letter_test2_bi = entropy_calc(letter_dict_test2_bi)\n",
    "entropy_jamo_test2_bi = entropy_calc(jamo_dict_test2_bi)\n",
    "\n",
    "cross_entropy_word_test2_bi = cross_entropy_calc(word_dict_test2_bi, word_dict_train_bi)\n",
    "cross_entropy_letter_test2_bi = cross_entropy_calc(letter_dict_test2_bi, letter_dict_train_bi)\n",
    "cross_entropy_jamo_test2_bi = cross_entropy_calc(jamo_dict_test2_bi, jamo_dict_train_bi)\n",
    "\n",
    "diff_word_bi_test2 = cross_entropy_word_test2_bi - entropy_word_test2_bi\n",
    "diff_letter_bi_test2 = cross_entropy_letter_test2_bi - entropy_letter_test2_bi\n",
    "diff_jamo_bi_test2 = cross_entropy_jamo_test2_bi - entropy_jamo_test2_bi\n",
    "\n",
    "#trigram\n",
    "entropy_word_test2_tri = entropy_calc(word_dict_test2_tri)\n",
    "entropy_letter_test2_tri = entropy_calc(letter_dict_test2_tri)\n",
    "entropy_jamo_test2_tri = entropy_calc(jamo_dict_test2_tri)\n",
    "\n",
    "cross_entropy_word_test2_tri = cross_entropy_calc(word_dict_test2_tri, word_dict_train_tri)\n",
    "cross_entropy_letter_test2_tri = cross_entropy_calc(letter_dict_test2_tri, letter_dict_train_tri)\n",
    "cross_entropy_jamo_test2_tri = cross_entropy_calc(jamo_dict_test2_tri, jamo_dict_train_tri)\n",
    "\n",
    "diff_word_tri_test2 = cross_entropy_word_test2_tri - entropy_word_test2_tri\n",
    "diff_letter_tri_test2 = cross_entropy_letter_test2_tri - entropy_letter_test2_tri\n",
    "diff_jamo_tri_test2 = cross_entropy_jamo_test2_tri - entropy_jamo_test2_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "col = [\"Entropy\",\"Cross Entropy\",\"Difference H(p,m)-H(p)\"]\n",
    "ind = [[\"Training\",\"Training\",\"Training\",\n",
    "        \"Training\",\"Training\",\"Training\",\n",
    "        \"Training\",\"Training\",\"Training\",\n",
    "        \"Test: 뉴스기사\",\"Test: 뉴스기사\",\"Test: 뉴스기사\",\n",
    "        \"Test: 뉴스기사\",\"Test: 뉴스기사\",\"Test: 뉴스기사\",\n",
    "        \"Test: 뉴스기사\",\"Test: 뉴스기사\",\"Test: 뉴스기사\",\n",
    "        \"Test: 웹 기사\",\"Test: 웹 기사\",\"Test: 웹 기사\",\n",
    "        \"Test: 웹 기사\",\"Test: 웹 기사\",\"Test: 웹 기사\",\n",
    "        \"Test: 웹 기사\",\"Test: 웹 기사\",\"Test: 웹 기사\"],\n",
    "       [\"자소별\",\"자소별\",\"자소별\",\"글자별\",\"글자별\",\"글자별\",\"어절별\",\"어절별\",\"어절별\",\n",
    "       \"자소별\",\"자소별\",\"자소별\",\"글자별\",\"글자별\",\"글자별\",\"어절별\",\"어절별\",\"어절별\",\n",
    "       \"자소별\",\"자소별\",\"자소별\",\"글자별\",\"글자별\",\"글자별\",\"어절별\",\"어절별\",\"어절별\"],\n",
    "      [\"unigram\", \"bigram\", \"trigram\",\"unigram\", \"bigram\", \"trigram\",\"unigram\", \"bigram\", \"trigram\",\n",
    "      \"unigram\", \"bigram\", \"trigram\",\"unigram\", \"bigram\", \"trigram\",\"unigram\", \"bigram\", \"trigram\",\n",
    "      \"unigram\", \"bigram\", \"trigram\",\"unigram\", \"bigram\", \"trigram\",\"unigram\", \"bigram\", \"trigram\"]]\n",
    "con = [[entropy_jamo_train_uni, cross_entropy_jamo_train_uni, diff_jamo_uni_train],\n",
    "       [entropy_jamo_train_bi, cross_entropy_jamo_train_bi,diff_jamo_bi_train],\n",
    "       [entropy_jamo_train_tri, cross_entropy_jamo_train_tri,diff_jamo_tri_train],\n",
    "       [entropy_letter_train_uni, cross_entropy_letter_train_uni, diff_letter_uni_train],\n",
    "       [entropy_letter_train_bi, cross_entropy_letter_train_bi,diff_letter_bi_train],\n",
    "       [entropy_letter_train_tri, cross_entropy_letter_train_tri,diff_letter_tri_train],\n",
    "       [entropy_word_train_uni, cross_entropy_word_train_uni, diff_word_uni_train],\n",
    "       [entropy_word_train_bi, cross_entropy_word_train_bi,diff_word_bi_train],\n",
    "       [entropy_word_train_tri, cross_entropy_word_train_tri,diff_word_tri_train],\n",
    "       [entropy_jamo_test1_uni, cross_entropy_jamo_test1_uni, diff_jamo_uni_test1],\n",
    "       [entropy_jamo_test1_bi, cross_entropy_jamo_test1_bi,diff_jamo_bi_test1],\n",
    "       [entropy_jamo_test1_tri, cross_entropy_jamo_test1_tri,diff_jamo_tri_test1],\n",
    "       [entropy_letter_test1_uni, cross_entropy_letter_test1_uni, diff_letter_uni_test1],\n",
    "       [entropy_letter_test1_bi, cross_entropy_letter_test1_bi,diff_letter_bi_test1],\n",
    "       [entropy_letter_test1_tri, cross_entropy_letter_test1_tri,diff_letter_tri_test1],\n",
    "       [entropy_word_test1_uni, cross_entropy_word_test1_uni, diff_word_uni_test1],\n",
    "       [entropy_word_test1_bi, cross_entropy_word_test1_bi,diff_word_bi_test1],\n",
    "       [entropy_word_test1_tri, cross_entropy_word_test1_tri,diff_word_tri_test1],\n",
    "       [entropy_jamo_test2_uni, cross_entropy_jamo_test2_uni, diff_jamo_uni_test2],\n",
    "       [entropy_jamo_test2_bi, cross_entropy_jamo_test2_bi,diff_jamo_bi_test2],\n",
    "       [entropy_jamo_test2_tri, cross_entropy_jamo_test2_tri,diff_jamo_tri_test2],\n",
    "       [entropy_letter_test2_uni, cross_entropy_letter_test2_uni, diff_letter_uni_test2],\n",
    "       [entropy_letter_test2_bi, cross_entropy_letter_test2_bi,diff_letter_bi_test2],\n",
    "       [entropy_letter_test2_tri, cross_entropy_letter_test2_tri,diff_letter_tri_test2],\n",
    "       [entropy_word_test2_uni, cross_entropy_word_test2_uni, diff_word_uni_test2],\n",
    "       [entropy_word_test2_bi, cross_entropy_word_test2_bi,diff_word_bi_test2],\n",
    "       [entropy_word_test2_tri, cross_entropy_word_test2_tri,diff_word_tri_test2]]\n",
    "df = pd.DataFrame(con,columns=col,index=ind).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Difference H(p,m)-H(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Training</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">자소별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>4.37520</td>\n",
       "      <td>4.37520</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>7.49098</td>\n",
       "      <td>7.49098</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>9.80415</td>\n",
       "      <td>9.80415</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">글자별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>7.96269</td>\n",
       "      <td>7.96269</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>12.67227</td>\n",
       "      <td>12.67227</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>14.96964</td>\n",
       "      <td>14.96964</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">어절별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>14.50234</td>\n",
       "      <td>14.50234</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>17.89565</td>\n",
       "      <td>17.89565</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>18.22401</td>\n",
       "      <td>18.22401</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Test: 뉴스기사</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">자소별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>4.37371</td>\n",
       "      <td>4.37862</td>\n",
       "      <td>0.00491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>7.52037</td>\n",
       "      <td>7.55372</td>\n",
       "      <td>0.03334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>9.94462</td>\n",
       "      <td>10.07759</td>\n",
       "      <td>0.13297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">글자별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>8.09896</td>\n",
       "      <td>8.21390</td>\n",
       "      <td>0.11494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>13.34323</td>\n",
       "      <td>15.48966</td>\n",
       "      <td>2.14643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>15.12571</td>\n",
       "      <td>23.20193</td>\n",
       "      <td>8.07622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">어절별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>14.58292</td>\n",
       "      <td>22.91277</td>\n",
       "      <td>8.32985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>16.57606</td>\n",
       "      <td>41.27387</td>\n",
       "      <td>24.69781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>16.70616</td>\n",
       "      <td>45.30080</td>\n",
       "      <td>28.59464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Test: 웹 기사</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">자소별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>4.30931</td>\n",
       "      <td>4.33117</td>\n",
       "      <td>0.02186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>7.37975</td>\n",
       "      <td>7.56082</td>\n",
       "      <td>0.18107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>9.64585</td>\n",
       "      <td>10.23410</td>\n",
       "      <td>0.58825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">글자별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>7.86512</td>\n",
       "      <td>8.37407</td>\n",
       "      <td>0.50895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>12.06668</td>\n",
       "      <td>18.41191</td>\n",
       "      <td>6.34523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>13.51146</td>\n",
       "      <td>33.79051</td>\n",
       "      <td>20.27905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">어절별</th>\n",
       "      <th>unigram</th>\n",
       "      <td>13.30041</td>\n",
       "      <td>34.21432</td>\n",
       "      <td>20.91391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>15.28800</td>\n",
       "      <td>72.32415</td>\n",
       "      <td>57.03615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>15.40591</td>\n",
       "      <td>83.04685</td>\n",
       "      <td>67.64094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Entropy  Cross Entropy  Difference H(p,m)-H(p)\n",
       "Training   자소별 unigram   4.37520        4.37520                 0.00000\n",
       "               bigram    7.49098        7.49098                -0.00000\n",
       "               trigram   9.80415        9.80415                -0.00000\n",
       "           글자별 unigram   7.96269        7.96269                -0.00000\n",
       "               bigram   12.67227       12.67227                -0.00000\n",
       "               trigram  14.96964       14.96964                -0.00000\n",
       "           어절별 unigram  14.50234       14.50234                -0.00000\n",
       "               bigram   17.89565       17.89565                 0.00000\n",
       "               trigram  18.22401       18.22401                 0.00000\n",
       "Test: 뉴스기사 자소별 unigram   4.37371        4.37862                 0.00491\n",
       "               bigram    7.52037        7.55372                 0.03334\n",
       "               trigram   9.94462       10.07759                 0.13297\n",
       "           글자별 unigram   8.09896        8.21390                 0.11494\n",
       "               bigram   13.34323       15.48966                 2.14643\n",
       "               trigram  15.12571       23.20193                 8.07622\n",
       "           어절별 unigram  14.58292       22.91277                 8.32985\n",
       "               bigram   16.57606       41.27387                24.69781\n",
       "               trigram  16.70616       45.30080                28.59464\n",
       "Test: 웹 기사 자소별 unigram   4.30931        4.33117                 0.02186\n",
       "               bigram    7.37975        7.56082                 0.18107\n",
       "               trigram   9.64585       10.23410                 0.58825\n",
       "           글자별 unigram   7.86512        8.37407                 0.50895\n",
       "               bigram   12.06668       18.41191                 6.34523\n",
       "               trigram  13.51146       33.79051                20.27905\n",
       "           어절별 unigram  13.30041       34.21432                20.91391\n",
       "               bigram   15.28800       72.32415                57.03615\n",
       "               trigram  15.40591       83.04685                67.64094"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
